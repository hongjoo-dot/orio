"""
ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâÍ¥ëÍ≥† AD Î¶¨Ìè¨Ìä∏ ÏàòÏßë Î™®Îìà
AD + AD_CONVERSION Î¶¨Ìè¨Ìä∏ ÌÜµÌï© ÏàòÏßë
"""

import requests
import time
from datetime import datetime
from urllib.parse import urlparse
from .auth import NaverAuth

BASE_URL = "https://api.naver.com"


class NaverADReportFetcher:
    """AD + AD_CONVERSION Î¶¨Ìè¨Ìä∏ ÌÜµÌï© ÏàòÏßë"""

    def __init__(self):
        self.auth = NaverAuth()
        self.base_url = BASE_URL

    def fetch_ad_report_data(self, target_date: str) -> list:
        """
        ÌäπÏ†ï ÎÇ†ÏßúÏùò AD + AD_CONVERSION Î¶¨Ìè¨Ìä∏ Îç∞Ïù¥ÌÑ∞ ÌÜµÌï© ÏàòÏßë
        
        Args:
            target_date: ÎÇ†Ïßú (YYYY-MM-DD ÌòïÏãù)
        
        Returns:
            list: ÌååÏã±Îêú ÌÜµÌï© Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏
        """
        date_obj = datetime.strptime(target_date, '%Y-%m-%d')
        stat_dt = date_obj.strftime('%Y%m%d')

        print(f"\n[ÌÜµÌï© Î¶¨Ìè¨Ìä∏ ÏàòÏßë] {target_date}")

        # 1. AD Î¶¨Ìè¨Ìä∏ ÏàòÏßë
        ad_data = self._fetch_report('AD', stat_dt, target_date)

        # 2. AD_CONVERSION Î¶¨Ìè¨Ìä∏ ÏàòÏßë
        conversion_data = self._fetch_report('AD_CONVERSION', stat_dt, target_date)

        # 3. Îç∞Ïù¥ÌÑ∞ ÌÜµÌï©
        merged_data = self._merge_reports(ad_data, conversion_data, target_date)

        print(f"   [ÌÜµÌï© ÏôÑÎ£å] {len(merged_data)}Í±¥")

        return merged_data

    def _fetch_report(self, report_type: str, stat_dt: str, target_date: str) -> list:
        """ÌäπÏ†ï ÌÉÄÏûÖÏùò Î¶¨Ìè¨Ìä∏ ÏàòÏßë"""
        print(f"\n   üìä [{report_type}] Î¶¨Ìè¨Ìä∏ ÏàòÏßë ÏãúÏûë...")

        # 1. Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        report_job_id = self._create_report(report_type, stat_dt)
        if not report_job_id:
            print(f"      ‚ùå Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Îã®Í≥Ñ Ïã§Ìå®")
            return []

        # 2. Î¶¨Ìè¨Ìä∏ ÏôÑÎ£å ÎåÄÍ∏∞
        report_info = self._wait_for_report(report_job_id, max_wait=120)
        if not report_info or report_info.get('status') != 'BUILT':
            print(f"      ‚ùå Î¶¨Ìè¨Ìä∏ ÎπåÎìú Îã®Í≥Ñ Ïã§Ìå®")
            return []

        # 3. Îã§Ïö¥Î°úÎìú
        download_url = report_info.get('downloadUrl')
        if not download_url:
            print(f"      ‚ùå Îã§Ïö¥Î°úÎìú URL ÏóÜÏùå")
            print(f"         report_info: {report_info}")
            return []

        tsv_data = self._download_report(download_url)
        if not tsv_data:
            print(f"      ‚ùå Îã§Ïö¥Î°úÎìú Îã®Í≥Ñ Ïã§Ìå®")
            return []

        # 4. ÌååÏã±
        if report_type == 'AD':
            parsed_data = self._parse_ad_tsv(tsv_data, target_date)
        else:
            parsed_data = self._parse_conversion_tsv(tsv_data, target_date)

        print(f"      ‚úì ÌååÏã± ÏôÑÎ£å: {len(parsed_data)}Í±¥")

        return parsed_data

    def _create_report(self, report_type: str, stat_dt: str):
        """Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏöîÏ≤≠"""
        uri = '/stat-reports'
        headers = self.auth.get_headers('POST', uri)

        report_data = {
            'reportTp': report_type,
            'statDt': stat_dt
        }

        try:
            response = requests.post(
                f"{self.base_url}{uri}",
                headers=headers,
                json=report_data,
                timeout=10
            )

            if response.status_code in [200, 201]:
                result = response.json()
                report_job_id = result.get('reportJobId')
                print(f"   ‚úì Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±: {report_job_id}")
                return report_job_id
            else:
                print(f"   ‚ùå Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {response.status_code}")
                print(f"      ÏùëÎãµ: {response.text}")
                return None

        except Exception as e:
            print(f"   ‚ùå Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏòàÏô∏: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _wait_for_report(self, report_job_id: str, max_wait: int = 120):
        """Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å ÎåÄÍ∏∞"""
        uri = f'/stat-reports/{report_job_id}'
        wait_interval = 3

        print(f"   ‚è≥ Î¶¨Ìè¨Ìä∏ ÎπåÎìú ÎåÄÍ∏∞ Ï§ë (ÏµúÎåÄ {max_wait}Ï¥à)...")

        for elapsed in range(0, max_wait, wait_interval):
            time.sleep(wait_interval)

            headers = self.auth.get_headers('GET', uri)

            try:
                response = requests.get(
                    f"{self.base_url}{uri}",
                    headers=headers,
                    timeout=10
                )

                if response.status_code == 200:
                    report_info = response.json()
                    status = report_info.get('status')

                    if status == 'BUILT':
                        print(f"   ‚úì Î¶¨Ìè¨Ìä∏ ÏôÑÎ£å ({elapsed + wait_interval}Ï¥à)")
                        return report_info
                    elif status == 'FAIL':
                        print(f"   ‚ùå Î¶¨Ìè¨Ìä∏ ÎπåÎìú Ïã§Ìå® (status=FAIL)")
                        print(f"      ÏùëÎãµ: {report_info}")
                        return None
                    elif status in ['REGIST', 'RUNNING']:
                        # ÏßÑÌñâ Ï§ë
                        if (elapsed + wait_interval) % 15 == 0:
                            print(f"      ÏßÑÌñâ Ï§ë... ({elapsed + wait_interval}Ï¥à Í≤ΩÍ≥º, status={status})")
                else:
                    print(f"   ‚ùå ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {response.status_code}")
                    print(f"      ÏùëÎãµ: {response.text}")

            except Exception as e:
                print(f"   ‚ö†Ô∏è  ÏÉÅÌÉú Ï°∞Ìöå ÏòàÏô∏: {e}")
                pass

        print(f"   ‚ùå ÌÉÄÏûÑÏïÑÏõÉ ({max_wait}Ï¥à) - Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±Ïù¥ ÏôÑÎ£åÎêòÏßÄ ÏïäÏùå")
        return None

    def _download_report(self, download_url: str):
        """Î¶¨Ìè¨Ìä∏ Îã§Ïö¥Î°úÎìú"""
        try:
            parsed = urlparse(download_url)
            download_uri = parsed.path

            headers = self.auth.get_headers('GET', download_uri)

            response = requests.get(
                download_url,
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                print(f"   ‚úì Îã§Ïö¥Î°úÎìú ÏôÑÎ£å ({len(response.text)} bytes)")
                return response.text
            else:
                print(f"   ‚ùå Îã§Ïö¥Î°úÎìú Ïã§Ìå®: {response.status_code}")
                print(f"      ÏùëÎãµ: {response.text}")
                return None

        except Exception as e:
            print(f"   ‚ùå Îã§Ïö¥Î°úÎìú ÏòàÏô∏: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _parse_ad_tsv(self, tsv_data: str, target_date: str) -> list:
        """AD Î¶¨Ìè¨Ìä∏ TSV Îç∞Ïù¥ÌÑ∞ ÌååÏã±"""
        lines = tsv_data.strip().split('\n')
        parsed_data = []

        for line in lines:
            if not line.strip():
                continue

            columns = line.split('\t')
            if len(columns) < 14:
                continue

            try:
                key = f"{columns[2]}_{columns[3]}_{columns[4]}_{columns[5]}_{columns[8]}"

                row = {
                    'key': key,
                    'Date': target_date,
                    'CampaignID': columns[2],
                    'AdGroupID': columns[3],
                    'KeywordID': columns[4],
                    'AdID': columns[5],
                    'Device': columns[8],
                    'Impressions': int(columns[9]) if columns[9].isdigit() else 0,
                    'Clicks': int(columns[10]) if columns[10].isdigit() else 0,
                }

                parsed_data.append(row)

            except (ValueError, IndexError):
                continue

        return parsed_data

    def _parse_conversion_tsv(self, tsv_data: str, target_date: str) -> list:
        """AD_CONVERSION Î¶¨Ìè¨Ìä∏ TSV Îç∞Ïù¥ÌÑ∞ ÌååÏã±"""
        lines = tsv_data.strip().split('\n')
        parsed_data = []

        for line in lines:
            if not line.strip():
                continue

            columns = line.split('\t')
            if len(columns) < 13:
                continue

            try:
                key = f"{columns[2]}_{columns[3]}_{columns[4]}_{columns[5]}_{columns[8]}_{columns[10]}"

                row = {
                    'key': key,
                    'Date': target_date,
                    'CampaignID': columns[2],
                    'AdGroupID': columns[3],
                    'KeywordID': columns[4],
                    'AdID': columns[5],
                    'Device': columns[8],
                    'ConversionType': columns[10],
                    'Conversions': int(columns[11]) if columns[11].isdigit() else 0,
                    'ConversionValue': int(columns[12]) if columns[12].isdigit() else 0,
                }

                parsed_data.append(row)

            except (ValueError, IndexError):
                continue

        return parsed_data

    def _merge_reports(self, ad_data: list, conversion_data: list, target_date: str) -> list:
        """AD Î¶¨Ìè¨Ìä∏ÏôÄ AD_CONVERSION Î¶¨Ìè¨Ìä∏ ÌÜµÌï©"""
        ad_map = {item['key']: item for item in ad_data}

        conversion_map = {}
        for item in conversion_data:
            base_key = f"{item['CampaignID']}_{item['AdGroupID']}_{item['KeywordID']}_{item['AdID']}_{item['Device']}"

            if base_key not in conversion_map:
                conversion_map[base_key] = []

            conversion_map[base_key].append({
                'ConversionType': item['ConversionType'],
                'Conversions': item['Conversions'],
                'ConversionValue': item['ConversionValue']
            })

        merged = []

        for key, ad_item in ad_map.items():
            row = {
                'Date': target_date,
                'CampaignID': ad_item['CampaignID'],
                'AdGroupID': ad_item['AdGroupID'],
                'KeywordID': ad_item['KeywordID'],
                'AdID': ad_item['AdID'],
                'Device': ad_item['Device'],
                'Impressions': ad_item['Impressions'],
                'Clicks': ad_item['Clicks'],
            }

            # purchase Ï†ÑÌôòÎßå ÏßëÍ≥Ñ
            if key in conversion_map:
                conversions = conversion_map[key]
                purchase_conv = sum(c['Conversions'] for c in conversions if c['ConversionType'] == 'purchase')
                purchase_revenue = sum(c['ConversionValue'] for c in conversions if c['ConversionType'] == 'purchase')

                row['Conversions'] = purchase_conv
                row['ConversionValue'] = purchase_revenue
            else:
                row['Conversions'] = 0
                row['ConversionValue'] = 0

            merged.append(row)

        return merged
